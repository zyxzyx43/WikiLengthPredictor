{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 2023 COMP 4446 / 5046 Assignment 1\n",
        "\n",
        "Assingment 1 is an **individual** assessment. Please note the University's [Academic dishonesty and plagiarism policy](https://www.sydney.edu.au/students/academic-dishonesty.html).\n",
        "\n",
        "Submission Deadline: Friday, March 17th, 2023, 11:59pm\n",
        "\n",
        "Submit via Canvas:\n",
        "- Your notebook\n",
        "- Run all cells before saving the notebook, so we can see your output\n",
        "\n",
        "In this assignment, we will explore ways to predict the length of a Wikipedia article based on the first 100 tokens in the article. Such a model could be used to explore whether there are systematic biases in the types of articles that get more detail.\n",
        "\n",
        "If you are working in another language, please make sure to clearly indicate which part of your code is running which section of the assignment and produce output that provides all necessary information. Submit your code, example outputs, and instructions for executing it.\n",
        "\n",
        "Note: This assignment contains topics that are not covered at the time of release. Each section has information about which lectures and/or labs covered the relevant material. We are releasing it now so you can (1) start working on some parts early, and (2) know what will be in the assignment when you attend the relevant labs and lectures."
      ],
      "metadata": {
        "id": "UOKBV2uWZ9U_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TODO: Copy and Name this File**\n",
        "Make a copy of this notebook in your own Google Drive (File -> Save a Copy in Drive) and change the filename, replacing `YOUR-UNIKEY`. For example, for a person with unikey `mcol1997`, the filename should be:\n",
        "\n",
        "`COMP-4446-5046_Assignment1_mcol1997.ipynb`"
      ],
      "metadata": {
        "id": "FA3m7neId4ow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Readme\n",
        "*If there is something you want to tell the marker about your submission, please mention it here.* "
      ],
      "metadata": {
        "id": "qut4Xg5mbYXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[write here - optional]"
      ],
      "metadata": {
        "id": "YaX3ihzU7uDL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Download [DO NOT MODIFY THIS]\n",
        "\n",
        "We have already constructed a dataset for you using a recent dump of data from Wikipedia. Both the training and test datasets are provided in the form of csv files (training_data.csv, test_data.csv) and can be downloaded from Google Drive using the code below. Each row of the data contains:\n",
        "\n",
        "- The length of the article\n",
        "- The title of the article\n",
        "- The first 100 tokens of the article\n",
        "\n",
        "In case you are curious, we constructed this dataset as follows:\n",
        "1. Downloaded [a recent dump](https://dumps.wikimedia.org/) of English wikipedia.\n",
        "2. Ran [WikiExtractor](https://github.com/attardi/wikiextractor) to get the contents of the pages.\n",
        "3. Filtered out very short pages.\n",
        "4. Ran [SpaCy](https://spacy.io/) with the `en_core_web_lg` model to tokenise the pages (Note, SpaCy's development is led by an alumnus of USyd!).\n",
        "5. Counted the tokens and saved the relevant data in the format described above.\n",
        "\n",
        "This code will download the data. **DO NOT MODIFY IT**"
      ],
      "metadata": {
        "id": "-Ib68RAoatjk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94u_ipMMZ6Cu",
        "outputId": "cb3cf61e-a2cb-4046-918e-30f55ab885db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------\n",
            "Size of training data: 9859\n",
            "Size of development data: 994\n",
            "Size of test data: 991\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Sample Data\n",
            "LABEL: 6453 / SENTENCE: ['Anarchism', 'Anarchism is a political philosophy and movement that is skeptical of all justifications for authority and seeks to abolish the institutions it claims maintain unnecessary coercion and hierarchy , typically including , though not necessarily limited to , governments , nation states , and capitalism . Anarchism advocates for the replacement of the state with stateless societies or other forms of free associations . As a historically left - wing movement , usually placed on the farthest left of the political spectrum , it is usually described alongside communalism and libertarian Marxism as the libertarian wing ( libertarian socialism )']\n",
            "------------------------------------\n",
            "6453\n",
            "Anarchism\n",
            "Anarchism is a political philosophy and movement that is skeptical of all justifications for authori...\n",
            "\n",
            "3528\n",
            "Albedo\n",
            "Albedo (; ) is the measure of the diffuse reflection of solar radiation out of the total solar radia...\n",
            "\n",
            "1265\n",
            "A\n",
            "A , or a , is the first letter and the first vowel of the Latin alphabet , used in the modern Englis...\n",
            "\n",
            "11591\n",
            "Alabama\n",
            "Alabama ( ) is a state in the Southeastern region of the United States , bordered by Tennessee to th...\n",
            "\n",
            "5865\n",
            "Achilles\n",
            "In Greek mythology , Achilles ( ) or Achilleus ( ) was a hero of the Trojan War , the greatest of al...\n",
            "\n",
            "13412\n",
            "Abraham Lincoln\n",
            "Abraham Lincoln ( ; February 12 , 1809   – April 15 , 1865 ) was an American lawyer , politician , a...\n",
            "\n",
            "9485\n",
            "Aristotle\n",
            "Aristotle (; \" Aristotélēs \" , ; 384–322   BC ) was a Greek philosopher and polymath during the Clas...\n",
            "\n",
            "1683\n",
            "An American in Paris\n",
            "An American in Paris is a jazz - influenced orchestral piece by American composer George Gershwin fi...\n",
            "\n",
            "149\n",
            "Academy Award for Best Production Design\n",
            "The Academy Award for Best Production Design recognizes achievement for art direction in film . The ...\n",
            "\n",
            "7178\n",
            "Academy Awards\n",
            "The Academy Awards , better known as the Oscars , are awards for artistic and technical merit for th...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## DO NOT MODIFY THIS CODE\n",
        "# Code to download files into Colaboratory\n",
        "\n",
        "# Install the PyDrive library\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "# Import libraries for accessing Google Drive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Function to read the file, save it on the machine this colab is running on, and then read it in\n",
        "import csv\n",
        "def read_file(file_id, filename):\n",
        "  downloaded = drive.CreateFile({'id':file_id})\n",
        "  downloaded.GetContentFile(filename)\n",
        "  with open(filename) as src:\n",
        "    reader = csv.reader(src)\n",
        "    data = [r for r in reader]\n",
        "  return data\n",
        "\n",
        "# Calls to get the data\n",
        "# If you need to access the data directly (e.g., you are running experiments on a local machine), use these links:\n",
        "# - Training, https://drive.google.com/file/d/1-UGFS8D-qglAX-czU38KaM4jQVCoNe0W/view?usp=share_link\n",
        "# - Dev, https://drive.google.com/file/d/1RWMEf0mdJMTkWc7dvN0ioks8bjujqZaN/view?usp=share_link\n",
        "# - Test, https://drive.google.com/file/d/1YVPNzdIFSMmVPeLBP-gf5DOIed3oRFyB/view?usp=share_link\n",
        "training_data = read_file('1-UGFS8D-qglAX-czU38KaM4jQVCoNe0W', \"/content/training_data.csv\")\n",
        "dev_data = read_file('1RWMEf0mdJMTkWc7dvN0ioks8bjujqZaN', \"/content/dev_data.csv\")\n",
        "test_data = read_file('1YVPNzdIFSMmVPeLBP-gf5DOIed3oRFyB', \"/content/test_data.csv\")\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Size of training data: {0}\".format(len(training_data)))\n",
        "print(\"Size of development data: {0}\".format(len(dev_data)))\n",
        "print(\"Size of test data: {0}\".format(len(test_data)))\n",
        "print(\"------------------------------------\")\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Sample Data\")\n",
        "print(\"LABEL: {0} / SENTENCE: {1}\".format(training_data[0][0], training_data[0][1:]))\n",
        "print(\"------------------------------------\")\n",
        "\n",
        "# Preview of the data in the csv file, which has three columns: \n",
        "# (1) length of article, (2) title of the article, (3) first 100 words in the article\n",
        "for v in training_data[:10]:\n",
        "  print(\"{}\\n{}\\n{}\\n\".format(v[0], v[1], v[2][:100] + \"...\"))\n",
        "\n",
        "# Store the data in lists and mofidy the length value to be in [0, 1]\n",
        "training_lengths = [min(1.0, int(r[0])/10000) for r in training_data]\n",
        "training_text = [r[2] for r in training_data]\n",
        "\n",
        "dev_lengths = [min(1.0, int(r[0])/10000) for r in dev_data]\n",
        "dev_text = [r[2] for r in dev_data]\n",
        "\n",
        "test_lengths = [min(1.0, int(r[0])/10000) for r in test_data]\n",
        "test_text = [r[2] for r in test_data]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - Predicting article length from initial content\n",
        "\n",
        "This section relates to content from **the week 1 lecture and the week 2 lab**.\n",
        "\n",
        "In this section, you will implement training and evaluation of a linear model (as seen in the week 2 lab) to predict the length of a wikipedia article from its first 100 words. You will represent the text using a Bag of Words model (as seen in the week 1 lecture)."
      ],
      "metadata": {
        "id": "QwiKfKQtphIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Word Mapping [2pt]\n",
        "\n",
        "In the code block below, write code to go through the training data and for any word that occurs at least 10 times:\n",
        "- Assign it a unique ID (consecutive, starting at 0)\n",
        "- Place it in a dictionary that maps from the word to the ID"
      ],
      "metadata": {
        "id": "DAGSol9qHIj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import textwrap\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import re\n",
        "import numpy as np\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "\n",
        "sww = sw.words()\n",
        "def get_token(documents):\n",
        "  token_doc = []\n",
        "  for document in documents:\n",
        "    tokenized_doc = word_tokenize(document)\n",
        "    token_doc.append(tokenized_doc)\n",
        "  return token_doc\n",
        "\n",
        "def get_Frequency(documents):\n",
        "  DF = {}\n",
        "  for sentence in documents:\n",
        "  # get each unique word in the doc - and count the number of occurrences in the document\n",
        "    for term in np.unique(sentence):\n",
        "      if term in DF:\n",
        "        DF[term] += 1\n",
        "      else:\n",
        "        DF[term] = 1\n",
        "  return DF\n",
        "\n",
        "def generate_map(TF):\n",
        "  popular_words = {}\n",
        "  word_id = 0\n",
        "  # get each tokenised doc\n",
        "  for word, freq in TF.items():\n",
        "    if freq >= 10:\n",
        "      popular_words[word] = word_id\n",
        "      word_id += 1\n",
        "  return popular_words"
      ],
      "metadata": {
        "id": "nsNf7pa5a9J6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8423e877-4d8e-4bc4-aba6-3854a0385824"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "token_docs = get_token(training_text)\n",
        "DF = get_Frequency(token_docs)\n",
        "popular_words = generate_map(DF)\n",
        "\n",
        "# Dev\n",
        "token_docs_dev = get_token(dev_text)\n",
        "DF_dev = get_Frequency(token_docs_dev)\n",
        "popular_words_dev = generate_map(DF_dev)\n",
        "\n",
        "# Test\n",
        "token_docs_test = get_token(test_text)\n",
        "DF_test = get_Frequency(token_docs_test)\n",
        "popular_words_test = generate_map(DF_test)"
      ],
      "metadata": {
        "id": "YeHds5NshE-f"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Data to Bag-of-Words Tensors [2pt]\n",
        "\n",
        "In the code block below, write code to prepare the data in PyTorch tensors.\n",
        "\n",
        "The text should be converted to a bag of words (ie., a vector the length of the vocabulary in the mapping in the previous step, with counts of the words in the text)."
      ],
      "metadata": {
        "id": "8r3Ej4fBIKJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_bow(token_docs, word_to_id):\n",
        "    x_data = []\n",
        "    vocab_size = len(word_to_id)\n",
        "    for tokens in token_docs:\n",
        "        bow_vector = [0] * vocab_size\n",
        "        for token in tokens:\n",
        "            if token in word_to_id:\n",
        "                bow_vector[word_to_id[token]] += 1\n",
        "        x_data.append(bow_vector)\n",
        "    return x_data"
      ],
      "metadata": {
        "id": "E1q3UCZBSZDv"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "import torch\n",
        "\n",
        "# Train\n",
        "x_data = text_to_bow(token_docs, popular_words)\n",
        "x_data = torch.Tensor(x_data)\n",
        "y_data = torch.tensor(training_lengths).view(-1, 1)\n",
        "\n",
        "# Dev\n",
        "x_dev_data = text_to_bow(token_docs_dev, popular_words)\n",
        "x_dev_data = torch.Tensor(x_dev_data)\n",
        "y_dev_data = torch.tensor(dev_lengths).view(-1, 1)\n",
        "\n",
        "# Test\n",
        "x_test_data = text_to_bow(token_docs_test, popular_words)\n",
        "x_test_data = torch.Tensor(x_test_data)\n",
        "y_test_data = torch.tensor(test_lengths).view(-1, 1)"
      ],
      "metadata": {
        "id": "7Or-645qIKQC"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Model Creation [2pt]\n",
        "\n",
        "Construct a linear model with an SGD optimiser (we recommend a learning rate of `1e-4`) and mean squared error as the loss."
      ],
      "metadata": {
        "id": "nsb--KW_I_F7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "linearRegression =  nn.Linear(len(x_data[1]), 1)\n",
        "optimizer = torch.optim.SGD(linearRegression.parameters(), lr=1e-4)\n",
        "\n",
        "# Define the loss function\n",
        "loss_func = F.mse_loss\n",
        "# Calculate loss\n",
        "loss = loss_func(linearRegression(x_data), y_data)"
      ],
      "metadata": {
        "id": "JEzbIGe4I_QK"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Training [2pt]\n",
        "\n",
        "Write a loop to train your model for 100 epochs, printing performance on the dev set every 10 epochs."
      ],
      "metadata": {
        "id": "AjHulTA6JQ3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of timing\n",
        "\n",
        "def train_model(model, optimizer, epochs, x_data, y_data, x_test, y_test, display):\n",
        "  no_of_epochs = epochs\n",
        "  display_interval = 10\n",
        "  for epoch in range(no_of_epochs):\n",
        "    predictions = model(x_data)\n",
        "    loss = loss_func(predictions, y_data)\n",
        "    loss.backward()\n",
        "    optimizer.step() #call step() to automatically update the parameters through our defined optimizer, which can be called once after backward()\n",
        "    optimizer.zero_grad() #reset the gradient as what we did before\n",
        "    if display:\n",
        "      if epoch % display_interval == 0 :\n",
        "          # calculate the loss of the current model\n",
        "          predictions = model(x_test)\n",
        "          loss = loss_func(predictions, y_test)\n",
        "          print(\"Epoch:\", '%04d' % (epoch), \"dev loss=\", \"{:.8f}\".format(loss))      \n"
      ],
      "metadata": {
        "id": "8U_k48teQCJ4"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(linearRegression, optimizer, 100, x_data, y_data, x_dev_data, y_dev_data, True)"
      ],
      "metadata": {
        "id": "WZdODnGdJQ8w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0048db33-ea7e-4c57-871e-7ec416d29e17"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0000 dev loss= 0.16427095\n",
            "Epoch: 0010 dev loss= 0.13270222\n",
            "Epoch: 0020 dev loss= 0.11353304\n",
            "Epoch: 0030 dev loss= 0.10187028\n",
            "Epoch: 0040 dev loss= 0.09475305\n",
            "Epoch: 0050 dev loss= 0.09038949\n",
            "Epoch: 0060 dev loss= 0.08769500\n",
            "Epoch: 0070 dev loss= 0.08601298\n",
            "Epoch: 0080 dev loss= 0.08494583\n",
            "Epoch: 0090 dev loss= 0.08425264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Measure Accuracy [2pt]\n",
        "\n",
        "In the code block below, write code to evaluate your model on the test set."
      ],
      "metadata": {
        "id": "bCwG22mOoyJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "def mse(x1, x2):\n",
        "  diff = x1 - x2\n",
        "  return torch.sum(diff*diff)/diff.numel()\n",
        "\n",
        "def evaluate_model(model, x_data, y_data, x_test_data, y_test_data):\n",
        "  print(\"=========================================================\")\n",
        "  training_loss = mse(model(x_data), y_data)   \n",
        "  print(\"Optimised:\", \"training loss=\", \"{:.9f}\".format(training_loss.data))\n",
        "  testing_loss = loss_func(model(x_test_data), y_test_data) \n",
        "  print(\"Testing loss=\", \"{:.9f}\".format(testing_loss.data))\n",
        "  print(\"Absolute mean square loss difference:\", \"{:.9f}\".format(abs(\n",
        "        training_loss.data - testing_loss.data)))\n",
        "\n",
        "%time evaluate_model(linearRegression, x_data, y_data, x_test_data, y_test_data)"
      ],
      "metadata": {
        "id": "gs_yX-Gnoydf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed513252-53e8-4321-d86a-c0fd018ba415"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================\n",
            "Optimised: training loss= 0.083256312\n",
            "Testing loss= 0.074223973\n",
            "Absolute mean square loss difference: 0.009032339\n",
            "CPU times: user 33.5 ms, sys: 0 ns, total: 33.5 ms\n",
            "Wall time: 33.8 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6 Analyse the Model [2pt]\n",
        "\n",
        "In the code block below, write code to identify the 50 words with the highest weights and the 50 words with the lowest weights."
      ],
      "metadata": {
        "id": "1TE7CMqZoylt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract weights from the linear regression model\n",
        "weights = linearRegression.weight.detach().numpy().flatten()\n",
        "\n",
        "# Get indices of the 50 highest and lowest weights\n",
        "top_50_indices = weights.argsort()[-50:][::-1]\n",
        "bottom_50_indices = weights.argsort()[:50]\n",
        "\n",
        "# Map the indices back to the words in the vocabulary\n",
        "popular_words_items = list(popular_words.items())\n",
        "top_50_words = [popular_words_items[i] for i in top_50_indices]\n",
        "bottom_50_words = [popular_words_items[i] for i in bottom_50_indices]"
      ],
      "metadata": {
        "id": "T4bmSbhhoy7d"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Compare Data Storage Methods\n",
        "\n",
        "This section relates to content from **the week 1 lecture and the week 2 lab**.\n",
        "\n",
        "Implement a variant of the model with a sparse vector for your input bag of words (See https://pytorch.org/docs/stable/sparse.html for how to switch a vector to be sparse). Use the default sparse vector type (COO)."
      ],
      "metadata": {
        "id": "l5fxNtitbFck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "x_data_sparse = x_data.to_sparse()\n",
        "y_data_sparse = torch.tensor(training_lengths).view(-1, 1)\n",
        "\n",
        "# Dev\n",
        "x_dev_data_sparse = x_dev_data.to_sparse()\n",
        "y_dev_data_sparse = torch.tensor(dev_lengths).view(-1, 1)\n",
        "\n",
        "# Test\n",
        "x_test_data_sparse = x_test_data.to_sparse()\n",
        "y_test_data_sparse = torch.tensor(test_lengths).view(-1, 1)\n",
        "\n",
        "linearRegression_sparse =  nn.Linear(len(x_data_sparse[1]), 1)\n",
        "optimizer_sparse = torch.optim.SGD(linearRegression_sparse.parameters(), lr=1e-4)\n",
        "\n",
        "# Define the loss function\n",
        "loss_func = F.mse_loss\n",
        "# Calculate loss\n",
        "loss_sparse = loss_func(linearRegression_sparse(x_data_sparse), y_data_sparse)"
      ],
      "metadata": {
        "id": "jc7LbuE6bQjW"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Training and Test Speed [2pt]\n",
        "Compare the time it takes to train and test the new model with the time it takes to train and test the old model.\n",
        "\n",
        "You can time the execution of a line of code using `%time`.\n",
        "See [this guide](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/01.07-Timing-and-Profiling.ipynb#scrollTo=z1gyaC_PNZUB) for more on timing."
      ],
      "metadata": {
        "id": "HkAPEr91qBTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "linearRegression_comp =  nn.Linear(len(x_data[1]), 1)\n",
        "optimizer_comp = torch.optim.SGD(linearRegression.parameters(), lr=1e-4)\n",
        "loss_func_comp = F.mse_loss\n",
        "# Calculate loss\n",
        "loss = loss_func(linearRegression(x_data), y_data)\n"
      ],
      "metadata": {
        "id": "eTNvBJZXN7LO"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense model\n",
        "%time dense_test = train_model(linearRegression_comp, optimizer_comp, 100, x_data, y_data, x_test_data, y_test_data, False)\n",
        "# Sparse model\n",
        "%time sparse_test = train_model(linearRegression_comp, optimizer_comp, 100, x_data_sparse, y_data_sparse, x_test_data_sparse, y_test_data_sparse, False)"
      ],
      "metadata": {
        "id": "HnRzVlA9qBYm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d58e95ab-dd2b-4a76-8d8d-4d905c871ec3"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5.09 s, sys: 17.8 ms, total: 5.11 s\n",
            "Wall time: 8.07 s\n",
            "CPU times: user 3.91 s, sys: 9.38 ms, total: 3.92 s\n",
            "Wall time: 4.51 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Switch to Word Embeddings\n",
        "\n",
        "This section relates to content from **the week 2 lecture and the week 3 lab**.\n",
        "\n",
        "In this section, you will implement a model based on word2vec.\n",
        "\n",
        "1. Use word2vec to learn embeddings for the words in your data.\n",
        "2. Represent each input document as the average of the word vectors for the words it contains.\n",
        "3. Train a linear regression model."
      ],
      "metadata": {
        "id": "Whic_heibGEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "from gensim.models import Word2Vec\n",
        "wv_cbow_model = Word2Vec(sentences=token_docs, size=100, window=5, min_count=10, workers=2, sg=0)\n",
        "#The size could be tuned"
      ],
      "metadata": {
        "id": "nA-x3rwObQ6_"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def document_to_mean_vector(doc, model):\n",
        "    word_vectors = [model.wv[word] for word in doc if word in model.wv]\n",
        "    if not word_vectors:\n",
        "        return np.zeros(model.vector_size)\n",
        "    return np.mean(word_vectors, axis=0)\n",
        "\n",
        "x_data_w2v = [document_to_mean_vector(doc, wv_cbow_model) for doc in token_docs]\n",
        "x_data_w2v = torch.tensor(x_data_w2v)\n",
        "\n",
        "x_dev_data_w2v = [document_to_mean_vector(doc, wv_cbow_model) for doc in token_docs_dev]\n",
        "x_dev_data_w2v = torch.tensor(x_dev_data_w2v)\n",
        "\n",
        "x_test_data_w2v = [document_to_mean_vector(doc, wv_cbow_model) for doc in token_docs_test]\n",
        "x_test_data_w2v = torch.tensor(x_test_data_w2v)\n"
      ],
      "metadata": {
        "id": "MZPl6XIogc_l"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linearRegression_w2v = nn.Linear(x_data_w2v.shape[1], 1)\n",
        "optimizer_w2v = torch.optim.SGD(linearRegression_w2v.parameters(), lr=1e-4)\n",
        "loss_func_w2v = F.mse_loss\n",
        "\n",
        "train_model(linearRegression_w2v, optimizer_w2v, 100, x_data_w2v, y_data, x_dev_data_w2v, y_dev_data, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "___BlNj7iYcL",
        "outputId": "0905e033-2b8e-444d-bb76-37012f4786d1"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0000 dev loss= 0.33477914\n",
            "Epoch: 0010 dev loss= 0.32882005\n",
            "Epoch: 0020 dev loss= 0.32300001\n",
            "Epoch: 0030 dev loss= 0.31731585\n",
            "Epoch: 0040 dev loss= 0.31176430\n",
            "Epoch: 0050 dev loss= 0.30634236\n",
            "Epoch: 0060 dev loss= 0.30104688\n",
            "Epoch: 0070 dev loss= 0.29587504\n",
            "Epoch: 0080 dev loss= 0.29082385\n",
            "Epoch: 0090 dev loss= 0.28589055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Accuracy [1pt]\n",
        "\n",
        "Calculate the accuracy of your model."
      ],
      "metadata": {
        "id": "Yj4ogp1Rq3Tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "evaluate_model(linearRegression_w2v, x_data_w2v, y_data, x_test_data_w2v, y_test_data)"
      ],
      "metadata": {
        "id": "TWP19fZKq3Zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b739825-e671-4eae-9150-0d91cbc5c75c"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================\n",
            "Optimised: training loss= 0.281960368\n",
            "Testing loss= 0.266987771\n",
            "Absolute mean square loss difference: 0.014972597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Speed [1pt]\n",
        "\n",
        "Calcualte how long it takes your model to be evaluated."
      ],
      "metadata": {
        "id": "zVXaNbLlq3fJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "# Dense model\n",
        "%time evaluate_model(linearRegression_w2v, x_data_w2v, y_data, x_test_data_w2v, y_test_data)"
      ],
      "metadata": {
        "id": "zp8q-nZOq3ko",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbe8da24-e194-4147-b8b4-c849cb762f95"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================\n",
            "Optimised: training loss= 0.281960368\n",
            "Testing loss= 0.266987771\n",
            "Absolute mean square loss difference: 0.014972597\n",
            "CPU times: user 4.42 ms, sys: 0 ns, total: 4.42 ms\n",
            "Wall time: 4.66 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - Open-Ended Improvement\n",
        "\n",
        "This section relates to content from **the week 1, 2, and 3 lectures and the week 1, 2, and 3 labs**.\n",
        "\n",
        "This section is an open-ended opportunity to find ways to make your model more accurate and/or faster (e.g., use WordNet to generalise words, try different word features, other optimisers, etc).\n",
        "\n",
        "We encourage you to try several ideas to provide scope for comparisons.\n",
        "\n",
        "If none of your ideas work you can still get full marks for this section. You just need to justify the ideas and discuss why they may not have improved performance.\n"
      ],
      "metadata": {
        "id": "QOT_5vmFbGy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Ideas and Motivation [1pt]\n",
        "\n",
        "In **this** box, describe your ideas and why you think they will improve accuracy and/or speed.\n",
        "\n",
        "*Your answer goes here*\n",
        "1. Data Pre-processing:  \n",
        "In Task 1, we didn't perform any significant pre-processing on the data. Implementing a pre-processing approach could potentially improve model performance by reducing noise, such as punctuation, and capturing more meaningful features from the text.\n",
        "\n",
        "2. Hyperparameter Tuning:  \n",
        "Employing hyperparameter tuning techniques can help us identify the optimal combination of hyperparameters for our model, leading to improved accuracy and/or faster training times. By fine-tuning these hyperparameters, we can optimize model performance and ensure the most effective use of available resources."
      ],
      "metadata": {
        "id": "Zryd7CmcrIjU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Implementation [2pt]\n",
        "\n",
        "Implement your ideas"
      ],
      "metadata": {
        "id": "jn1aesgGrJSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#1. Data Pre-processing:\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(sw.words('english'))\n",
        "    word_tokens = word_tokenize(text)\n",
        "    lower_tokens = [t.lower() for t in word_tokens] # change all words to lower case.\n",
        "    filtered_text = [word for word in lower_tokens if word not in stop_words and word.isalnum()] # Remove stop words and punctuation\n",
        "    return filtered_text\n",
        "\n",
        "# Preprocess the texts in the datasets\n",
        "preprocessed_train_token = [preprocess_text(text) for text in training_text]\n",
        "preprocessed_dev_token = [preprocess_text(text) for text in dev_text]\n",
        "preprocessed_test_token = [preprocess_text(text) for text in test_text]\n"
      ],
      "metadata": {
        "id": "0NhoI8FRbRSO"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the word frequency that is more than 10"
      ],
      "metadata": {
        "id": "A3Tevw7ZcaPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "preprocessed_DF = get_Frequency(preprocessed_train_token)\n",
        "preprocessed_popular_words = generate_map(preprocessed_DF)\n",
        "\n",
        "# Dev\n",
        "preprocessed_DF_dev = get_Frequency(preprocessed_dev_token)\n",
        "preprocessed_popular_words_dev = generate_map(preprocessed_DF_dev)\n",
        "\n",
        "# Test\n",
        "preprocessed_DF_test = get_Frequency(preprocessed_test_token)\n",
        "preprocessed_popular_words_test = generate_map(preprocessed_DF_test)"
      ],
      "metadata": {
        "id": "4BlNIbGi0gpC"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "x_pre_data = text_to_bow(preprocessed_train_token, popular_words)\n",
        "x_pre_data = torch.Tensor(x_pre_data).to_sparse()\n",
        "y_data = torch.tensor(training_lengths).view(-1, 1)\n",
        "\n",
        "# Dev\n",
        "x_dev_pre_data = text_to_bow(preprocessed_dev_token, popular_words)\n",
        "x_dev_pre_data = torch.Tensor(x_dev_pre_data).to_sparse()\n",
        "y_dev_data = torch.tensor(dev_lengths).view(-1, 1)\n",
        "\n",
        "# Test\n",
        "x_test_pre_data = text_to_bow(preprocessed_test_token, popular_words)\n",
        "x_test_pre_data = torch.Tensor(x_test_pre_data).to_sparse()\n",
        "y_test_data = torch.tensor(test_lengths).view(-1, 1)\n"
      ],
      "metadata": {
        "id": "DeODLqZq1hyZ"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tuning process involves testing multiple learning rates and selecting the one that yields the lowest loss, ensuring that we achieve optimal performance in the future."
      ],
      "metadata": {
        "id": "3Ld3VUbDbQF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune(learning_rate, x_data, y_data, x_test, y_test):\n",
        "  linearRegression =  nn.Linear(len(x_data[1]), 1)\n",
        "  best_lr = 0\n",
        "  best_loss = 99999\n",
        "  for lr in learning_rate:\n",
        "    temp_lr = lr\n",
        "    optimizer = torch.optim.SGD(linearRegression.parameters(), lr=temp_lr)\n",
        "    loss_func = F.mse_loss\n",
        "    no_of_epochs = 50\n",
        "    display_interval = 10\n",
        "\n",
        "    for epoch in range(no_of_epochs):\n",
        "      predictions = linearRegression(x_data)\n",
        "      loss = loss_func(predictions, y_data)\n",
        "      loss.backward()\n",
        "      optimizer.step() #call step() to automatically update the parameters through our defined optimizer, which can be called once after backward()\n",
        "      optimizer.zero_grad() #reset the gradient as what we did before     \n",
        "    training_loss = mse(linearRegression(x_data), y_data)   \n",
        "    testing_loss = loss_func(linearRegression(x_test_data), y_test_data)\n",
        "    loss_diff = training_loss.data - testing_loss.data\n",
        "    if loss_diff < best_loss:\n",
        "      best_lr = lr\n",
        "      best_loss = loss_diff\n",
        "  return best_lr\n"
      ],
      "metadata": {
        "id": "RhWXyTGZ2d7T"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linearRegression_impro =  nn.Linear(len(x_data[1]), 1)\n",
        "lr_list = [1e-4, 1e-3, 1e-5]\n",
        "best_lr = fine_tune(lr_list, x_data, y_data, x_dev_data, y_dev_data)\n",
        "optimizer_impro = torch.optim.SGD(linearRegression.parameters(), lr=best_lr)\n",
        "# Define the loss function\n",
        "loss_func = F.mse_loss\n",
        "\n",
        "train_model(linearRegression_impro, optimizer_impro, 100, x_pre_data, y_data, x_dev_pre_data, y_dev_data, True)"
      ],
      "metadata": {
        "id": "MVQxD50i19e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26b78918-3ee7-4dad-dade-e47cb7668bd3"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0000 dev loss= 0.17559277\n",
            "Epoch: 0010 dev loss= 0.17559277\n",
            "Epoch: 0020 dev loss= 0.17559277\n",
            "Epoch: 0030 dev loss= 0.17559277\n",
            "Epoch: 0040 dev loss= 0.17559277\n",
            "Epoch: 0050 dev loss= 0.17559277\n",
            "Epoch: 0060 dev loss= 0.17559277\n",
            "Epoch: 0070 dev loss= 0.17559277\n",
            "Epoch: 0080 dev loss= 0.17559277\n",
            "Epoch: 0090 dev loss= 0.17559277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Evaluation [1pt]\n",
        "\n",
        "Evaluate the speed and accuracy of the model with your ideas"
      ],
      "metadata": {
        "id": "btzsdyCTrW1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "%time improved_test = evaluate_model(linearRegression_impro, x_pre_data, y_data, x_test_pre_data, y_test_data)"
      ],
      "metadata": {
        "id": "nwJH-lB5rW-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b20df700-4e8e-43cd-e70e-521c12c27783"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================\n",
            "Optimised: training loss= 0.176300362\n",
            "Testing loss= 0.161845624\n",
            "Absolute mean square loss difference: 0.014454737\n",
            "CPU times: user 16.9 ms, sys: 1 ms, total: 17.9 ms\n",
            "Wall time: 15.7 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **this** text box, briefly describe the results. Did your improvement work? Why / Why not?\n",
        "\n",
        "The improvements implemented in Task 4 did not lead to a consistent increase in performance in terms of evaluation speed and accuracy. The evaluation speed for Task 1, Task 3, and Task 4 were 33.8ms, 4.66ms, and 15.7ms, respectively. While the evaluation speed improved from Task 1 to Task 3, it decreased from Task 3 to Task 4. The accuracy, as measured by the absolute mean square loss, was 0.009032339 for Task 1, 0.014972597 for Task 3, and 0.014454737 for Task 4. The accuracy worsened from Task 1 to Task 3 and remained relatively consistent from Task 3 to Task 4.\n",
        "\n",
        "The improvements may not have worked as intended due to the choice of hyperparameters or data pre-processing techniques. These changes could have unintentionally introduced noise or failed to capture meaningful patterns in the data. Additionally, the choice of a smaller learning rate may have impacted the model's convergence rate, leading to a decrease in evaluation speed.\n"
      ],
      "metadata": {
        "id": "xBTz8zwRRLVQ"
      }
    }
  ]
}